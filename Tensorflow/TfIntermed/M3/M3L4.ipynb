{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Generation With Recurrent Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load and preprocess data\n",
    "2. Tokenization and vocabulary creation\n",
    "3. Pad Sequences\n",
    "4. Split the data into input and target\n",
    "5. Build the RNN model\n",
    "6. Compile and train the model\n",
    "7. Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import requests\n",
    "import string\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = requests.get('https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "text = data.text.lower()\n",
    "# Remove punctuation\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "# Create a list of sentences\n",
    "sentences = text.split('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and vocabulary creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = []\n",
    "for sentence in sentences:\n",
    "    token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = np.array(tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_sequence_len))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into input and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = input_sequences[:, :-1]\n",
    "x_train = input_sequences[:, :-1]\n",
    "y_train = input_sequences[:, -1]\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=total_words)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(x_train.shape[0], 100, input_length=max_sequence_len-1),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150, return_sequences=True)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.LSTM(100),\n",
    "    tf.keras.layers.Dense(total_words, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5309/5309 [==============================] - 2070s 388ms/step - loss: 5.7117 - accuracy: 0.1875\n",
      "Epoch 2/100\n",
      "5309/5309 [==============================] - 2075s 391ms/step - loss: 5.3028 - accuracy: 0.2063\n",
      "Epoch 3/100\n",
      "5309/5309 [==============================] - 2093s 394ms/step - loss: 5.0974 - accuracy: 0.2198\n",
      "Epoch 4/100\n",
      "5309/5309 [==============================] - 2089s 393ms/step - loss: 4.9423 - accuracy: 0.2249\n",
      "Epoch 5/100\n",
      "5309/5309 [==============================] - 2091s 394ms/step - loss: 4.8168 - accuracy: 0.2299\n",
      "Epoch 6/100\n",
      "5309/5309 [==============================] - 2086s 393ms/step - loss: 4.7062 - accuracy: 0.2349\n",
      "Epoch 7/100\n",
      "5309/5309 [==============================] - 2103s 396ms/step - loss: 4.6058 - accuracy: 0.2395\n",
      "Epoch 8/100\n",
      "5309/5309 [==============================] - 2092s 394ms/step - loss: 4.5115 - accuracy: 0.2436\n",
      "Epoch 9/100\n",
      "5309/5309 [==============================] - 2091s 394ms/step - loss: 4.4232 - accuracy: 0.2478\n",
      "Epoch 10/100\n",
      "5309/5309 [==============================] - 2088s 393ms/step - loss: 4.3425 - accuracy: 0.2521\n",
      "Epoch 11/100\n",
      "5309/5309 [==============================] - 2091s 394ms/step - loss: 4.2688 - accuracy: 0.2560\n",
      "Epoch 12/100\n",
      "5309/5309 [==============================] - 2088s 393ms/step - loss: 4.1992 - accuracy: 0.2613\n",
      "Epoch 13/100\n",
      "5309/5309 [==============================] - 2099s 395ms/step - loss: 4.1365 - accuracy: 0.2670\n",
      "Epoch 14/100\n",
      "5309/5309 [==============================] - 2092s 394ms/step - loss: 4.0793 - accuracy: 0.2720\n",
      "Epoch 15/100\n",
      "5309/5309 [==============================] - 2090s 394ms/step - loss: 4.0213 - accuracy: 0.2774\n",
      "Epoch 16/100\n",
      "5309/5309 [==============================] - 2095s 395ms/step - loss: 3.9657 - accuracy: 0.2838\n",
      "Epoch 17/100\n",
      "5309/5309 [==============================] - 2092s 394ms/step - loss: 3.9139 - accuracy: 0.2898\n",
      "Epoch 18/100\n",
      "5309/5309 [==============================] - 2170s 409ms/step - loss: 3.8664 - accuracy: 0.2945\n",
      "Epoch 19/100\n",
      "5309/5309 [==============================] - 2095s 395ms/step - loss: 3.8201 - accuracy: 0.3007\n",
      "Epoch 20/100\n",
      "5309/5309 [==============================] - 2097s 395ms/step - loss: 3.7762 - accuracy: 0.3059\n",
      "Epoch 21/100\n",
      "5309/5309 [==============================] - 2101s 396ms/step - loss: 3.7356 - accuracy: 0.3110\n",
      "Epoch 22/100\n",
      "5309/5309 [==============================] - 2102s 396ms/step - loss: 3.6968 - accuracy: 0.3152\n",
      "Epoch 23/100\n",
      "5309/5309 [==============================] - 2102s 396ms/step - loss: 3.6596 - accuracy: 0.3199\n",
      "Epoch 24/100\n",
      "5309/5309 [==============================] - 2108s 397ms/step - loss: 3.6229 - accuracy: 0.3259\n",
      "Epoch 25/100\n",
      "5309/5309 [==============================] - 2101s 396ms/step - loss: 3.5863 - accuracy: 0.3294\n",
      "Epoch 26/100\n",
      "5309/5309 [==============================] - 2100s 395ms/step - loss: 3.5541 - accuracy: 0.3347\n",
      "Epoch 27/100\n",
      "5309/5309 [==============================] - 2101s 396ms/step - loss: 3.5231 - accuracy: 0.3392\n",
      "Epoch 28/100\n",
      "5309/5309 [==============================] - 2099s 395ms/step - loss: 3.4916 - accuracy: 0.3454\n",
      "Epoch 29/100\n",
      "5309/5309 [==============================] - 2099s 395ms/step - loss: 3.4625 - accuracy: 0.3483\n",
      "Epoch 30/100\n",
      "5309/5309 [==============================] - 2102s 396ms/step - loss: 3.4332 - accuracy: 0.3529\n",
      "Epoch 31/100\n",
      "5309/5309 [==============================] - 2101s 396ms/step - loss: 3.4071 - accuracy: 0.3564\n",
      "Epoch 32/100\n",
      "5309/5309 [==============================] - 2100s 396ms/step - loss: 3.3844 - accuracy: 0.3596\n",
      "Epoch 33/100\n",
      "5309/5309 [==============================] - 2128s 401ms/step - loss: 3.3578 - accuracy: 0.3643\n",
      "Epoch 34/100\n",
      "5309/5309 [==============================] - 2127s 401ms/step - loss: 3.3325 - accuracy: 0.3682\n",
      "Epoch 35/100\n",
      "5309/5309 [==============================] - 2126s 400ms/step - loss: 3.3083 - accuracy: 0.3713\n",
      "Epoch 36/100\n",
      "5309/5309 [==============================] - 2118s 399ms/step - loss: 3.2896 - accuracy: 0.3748\n",
      "Epoch 37/100\n",
      "3622/5309 [===================>..........] - ETA: 11:38 - loss: 3.2373 - accuracy: 0.3840"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, epochs=100, verbose=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = \"to be or not to\"\n",
    "next_words = 100\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list], maxlen=max_sequence_len-1)\n",
    "    predicted = model.predict_classes(token_list, verbose=0)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TfIntm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
