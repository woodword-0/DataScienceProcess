{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Text Classification Model "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preparing Text Data\n",
    "2. Creating the Model\n",
    "3. Compiling and Training the Model\n",
    "4. Making Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Example text data and labels\n",
    "\n",
    "texts = [\"I love this movie!\", \"This movie is terrible.\", \"Great book!\", \"Awful experience.\"]\n",
    "\n",
    "labels = np.array([1, 0, 1, 0])  # Positive sentiment: 1, Negative sentiment: 0\n",
    "processed_texts = []\n",
    "for text in texts:\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # POS tagging\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    # Remove stop words and retain relevant POS tags\n",
    "    filtered_tokens = [word for word, tag in tagged_tokens if word not in stop_words and tag.startswith(('JJ', 'NN'))]\n",
    "    # Join filtered tokens back into a sentence\n",
    "    processed_text = ' '.join(filtered_tokens)\n",
    "    processed_texts.append(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Tokenization\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(processed_texts)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(processed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 2],\n",
       "       [3, 4],\n",
       "       [5, 6]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Padding\n",
    "\n",
    "max_len = max(len(sequence) for sequence in sequences)\n",
    "\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "padded_sequences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, 2, 16)             112       \n",
      "                                                                 \n",
      " global_average_pooling1d_6   (None, 16)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 129\n",
      "Trainable params: 129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model architecture\n",
    "\n",
    "model = Sequential([\n",
    "\n",
    "        Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=16, input_length=max_len),\n",
    "\n",
    "            GlobalAveragePooling1D(),\n",
    "\n",
    "                Dense(units=1, activation='sigmoid')])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling and Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 5s 7ms/step - loss: 0.6962 - accuracy: 0.2500 \n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.6938 - accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6909 - accuracy: 0.7500\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6885 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.6864 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6838 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.6813 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6787 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.6762 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6736 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25a7f181e10>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling the model\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "\n",
    "model.fit(padded_sequences, labels, epochs=10, batch_size=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example new texts for prediction\n",
    "\n",
    "new_texts = [\"This movie is amazing!\", \"I didn't like the book.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the new texts\n",
    "processed_new_texts = []\n",
    "for text in new_texts:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    filtered_tokens = [word for word, tag in tagged_tokens if word not in stop_words and tag.startswith(('JJ', 'NN'))]\n",
    "    processed_text = ' '.join(filtered_tokens)\n",
    "    processed_new_texts.append(processed_text)\n",
    "\n",
    "# Tokenize and pad the new texts\n",
    "new_sequences = tokenizer.texts_to_sequences(processed_new_texts)\n",
    "new_padded_sequences = pad_sequences(new_sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 523ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "\n",
    "predictions = model.predict(new_padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: This movie is amazing!\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: I didn't like the book.\n",
      "Sentiment: Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the predictions\n",
    "for text, prediction in zip(new_texts, predictions):\n",
    "    sentiment = 'Positive' if prediction > 0.5 else 'Negative'\n",
    "    print(f'Text: {text}\\nSentiment: {sentiment}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TfIntm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
