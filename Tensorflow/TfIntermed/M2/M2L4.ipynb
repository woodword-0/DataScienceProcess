{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dataset\n",
    "# Define the input sequences and their labels\n",
    "# Generate all permutations (Number of ways to label 4 vertices of a square 1-4)\n",
    "X = list(itertools.permutations([1, 2, 3, 4]))\n",
    "# Generate labels to classify each labeling as a specific type\n",
    "labels = random.choices([0, 1, 2, 3], k=24)\n",
    "# Create a train/test set\n",
    "X = np.array(X)\n",
    "y = np.array(labels)\n",
    "X_train = X[:int(0.8*len(X))]\n",
    "y_train = y[:int(0.8*len(y))]\n",
    "X_test = X[int(0.8*len(X)):]\n",
    "y_test = y[int(0.8*len(y)):]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding RNN Variants"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Simple RNN\n",
    "2. LSTM\n",
    "3. GRU\n",
    "4. Comparison of variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RNN\n",
    "1. Basic variant of RNNs, also known as the vanilla RNN.\n",
    "2. It processes sequential data by maintaining a hidden state that is updated at each time step.\n",
    "3. SimpleRNN suffers from the vanishing gradient problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SimpleRNN model\n",
    "\n",
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Embedding(5, 8, input_length=4))  # Embedding layer to represent input sequences\n",
    "\n",
    "model1.add(SimpleRNN(16))  # SimpleRNN layer to capture sequential information\n",
    "\n",
    "model1.add(Dense(4, activation='softmax'))  # Output layer with softmax activation for classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (Long Short-Term Memory):\n",
    "1. Advanced RNN variant designed to overcome the vanishing gradient problem.\n",
    "2. Introduces memory cells and gating mechanisms to retain or forget information over time.\n",
    "3. Effective in capturing long-term dependencies in sequential data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Embedding(5, 8, input_length=4))  # Embedding layer to represent input sequences\n",
    "\n",
    "model2.add(tf.keras.layers.LSTM(16))  # SimpleRNN layer to capture sequential information\n",
    "\n",
    "model2.add(Dense(4, activation='softmax'))  # Output layer with softmax activation for classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU (Gated Recurrent Unit):\n",
    "1. Another RNN variant that addresses the vanishing gradient problem.\n",
    "2. Combines memory cell and gating mechanisms but with a simplified architecture compared to LSTM.\n",
    "3. GRU has fewer parameters and is computationally more efficient than LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the GRU model\n",
    "\n",
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Embedding(5, 8, input_length=4))  # Embedding layer to represent input sequences\n",
    "\n",
    "model3.add(tf.keras.layers.GRU(16))  # SimpleRNN layer to capture sequential information\n",
    "\n",
    "model3.add(Dense(4, activation='softmax')) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile the models\n",
    "\n",
    "model1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model3.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models\n",
    "history1 = model1.fit(X, y, epochs=10, validation_split=0.2)\n",
    "history2 = model2.fit(X, y, epochs=10, validation_split=0.2)\n",
    "history3 = model3.fit(X, y, epochs=10, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy of each model\n",
    "train_accuracy1 = history1.history['accuracy']\n",
    "train_accuracy2 = history2.history['accuracy']\n",
    "train_accuracy3 = history3.history['accuracy']\n",
    "best1 = train_accuracy1[np.argmax(train_accuracy1)]\n",
    "best2 = train_accuracy2[np.argmax(train_accuracy2)]\n",
    "best3 = train_accuracy3[np.argmax(train_accuracy3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla Rnn: 0.42105263471603394,LSTM: 0.31578946113586426,GRU: 0.31578946113586426\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compare RNN variants\n",
    "print(f'Vanilla Rnn: {best1},LSTM: {best2},GRU: {best3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing training scores\n",
    "# train_loss1 = history1.history['loss']\n",
    "# train_loss2 = history2.history['loss']\n",
    "# train_loss3 = history3.history['loss']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TfIntm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
